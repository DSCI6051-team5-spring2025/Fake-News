{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
        "import requests\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import torch.nn as nn\n",
        "\n",
        "# ==============================\n",
        "# Load and Preprocess Dataset\n",
        "# ==============================\n",
        "\n",
        "# Load dataset from CSV\n",
        "data_str = \"/content/politifact_articles_with_images (1).csv\"\n",
        "\n",
        "def load_dataset(file_path):\n",
        "    \"\"\"Load dataset from CSV or Excel.\"\"\"\n",
        "    if file_path.endswith('.csv'):\n",
        "        df = pd.read_csv(file_path)\n",
        "    elif file_path.endswith('.xlsx'):\n",
        "        df = pd.read_excel(file_path)\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported file format. Use CSV or Excel.\")\n",
        "\n",
        "    return df\n",
        "\n",
        "# Preprocess the dataset\n",
        "# Preprocess the dataset\n",
        "def preprocess_dataset(df):\n",
        "    # Label mapping\n",
        "    real_labels = ['true', 'mostly true', 'half true']\n",
        "    fake_labels = ['false', 'mostly false', 'pants on fire']\n",
        "\n",
        "    # Function to map labels to binary\n",
        "    def map_to_binary(rating):\n",
        "        rating = str(rating).strip().lower()\n",
        "        if rating in real_labels:\n",
        "            return 1  # Real news\n",
        "        elif rating in fake_labels:\n",
        "            return 0  # Fake news\n",
        "        else:\n",
        "            return None  # Filter out invalid labels\n",
        "\n",
        "    # Apply label mapping\n",
        "    df['Label'] = df['Rating'].apply(map_to_binary)\n",
        "\n",
        "    # Remove rows with invalid labels\n",
        "    df = df.dropna(subset=['Label'])\n",
        "\n",
        "    # Ensure label type is integer\n",
        "    df['Label'] = df['Label'].astype(int)\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "# Download images\n",
        "def download_images(df, image_dir='./images'):\n",
        "    \"\"\"Download images from URLs and save them locally.\"\"\"\n",
        "    os.makedirs(image_dir, exist_ok=True)\n",
        "    image_paths = []\n",
        "\n",
        "    for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Downloading images\"):\n",
        "        image_url = row.get('Image URL', None)\n",
        "\n",
        "        if pd.isna(image_url) or image_url is None or image_url == 'N/A' or image_url == 'nan':\n",
        "            image_paths.append(None)\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            image_filename = f\"{idx}.jpg\"\n",
        "            image_path = os.path.join(image_dir, image_filename)\n",
        "\n",
        "            if not os.path.exists(image_path):\n",
        "                response = requests.get(image_url)\n",
        "                img = Image.open(BytesIO(response.content))\n",
        "                img.save(image_path)\n",
        "\n",
        "            image_paths.append(image_path)\n",
        "        except Exception as e:\n",
        "            print(f\"Error downloading image {image_url}: {e}\")\n",
        "            image_paths.append(None)\n",
        "\n",
        "    df['local_image_path'] = image_paths\n",
        "    return df\n",
        "\n",
        "\n",
        "# ==============================\n",
        "# CLIP Classifier\n",
        "# ==============================\n",
        "\n",
        "class CLIPClassifier(nn.Module):\n",
        "    def __init__(self, clip_model_name=\"openai/clip-vit-base-patch32\", num_classes=2):\n",
        "        super(CLIPClassifier, self).__init__()\n",
        "        self.clip = CLIPModel.from_pretrained(clip_model_name)\n",
        "\n",
        "        # Freeze CLIP parameters\n",
        "        for param in self.clip.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        hidden_size = self.clip.config.projection_dim\n",
        "\n",
        "        # Classification head\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(hidden_size, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(512, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, input_ids=None, attention_mask=None, pixel_values=None, return_loss=False, labels=None):\n",
        "        outputs = self.clip(input_ids=input_ids, attention_mask=attention_mask, pixel_values=pixel_values)\n",
        "\n",
        "        text_embeds = outputs.text_embeds\n",
        "        logits = self.classifier(text_embeds)\n",
        "\n",
        "        loss = None\n",
        "        if return_loss and labels is not None:\n",
        "            loss_fct = nn.CrossEntropyLoss()\n",
        "            loss = loss_fct(logits, labels)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "\n",
        "# ==============================\n",
        "# PyTorch Dataset\n",
        "# ==============================\n",
        "\n",
        "class FakeNewsDataset(Dataset):\n",
        "    def __init__(self, dataframe, processor, text_column='Claim', label_column='Label', image_column='local_image_path'):\n",
        "        self.dataframe = dataframe\n",
        "        self.processor = processor\n",
        "        self.text_column = text_column\n",
        "        self.label_column = label_column\n",
        "        self.image_column = image_column\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataframe)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.dataframe.iloc[idx]\n",
        "\n",
        "        text = row[self.text_column]\n",
        "        label = row[self.label_column]\n",
        "        image_path = row[self.image_column]\n",
        "\n",
        "        if pd.isna(image_path) or image_path is None:\n",
        "            image = Image.new('RGB', (224, 224), color='white')\n",
        "        else:\n",
        "            try:\n",
        "                image = Image.open(image_path).convert('RGB')\n",
        "            except:\n",
        "                image = Image.new('RGB', (224, 224), color='white')\n",
        "\n",
        "        inputs = self.processor(\n",
        "            text=text,\n",
        "            images=image,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            max_length=77\n",
        "        )\n",
        "\n",
        "        inputs = {k: v.squeeze(0) for k, v in inputs.items()}\n",
        "        inputs['labels'] = torch.tensor(label, dtype=torch.long)\n",
        "\n",
        "        return inputs\n",
        "\n",
        "\n",
        "# ==============================\n",
        "# Model Training\n",
        "# ==============================\n",
        "\n",
        "def train_vlm(model, train_loader, val_loader, device, num_epochs=10, learning_rate=5e-5):\n",
        "    model.to(device)\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    training_stats = {'train_loss': [], 'val_loss': [], 'val_acc': []}\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        total_train_loss = 0\n",
        "\n",
        "        for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Training\"):\n",
        "            batch = {k: v.to(device) for k, v in batch.items()}\n",
        "            labels = batch.pop('labels')\n",
        "\n",
        "            logits, _ = model(**batch, return_loss=False)\n",
        "            loss = criterion(logits, labels)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_train_loss += loss.item()\n",
        "\n",
        "        avg_train_loss = total_train_loss / len(train_loader)\n",
        "        training_stats['train_loss'].append(avg_train_loss)\n",
        "\n",
        "        model.eval()\n",
        "        total_val_loss = 0\n",
        "        all_preds, all_labels = [], []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Validation\"):\n",
        "                batch = {k: v.to(device) for k, v in batch.items()}\n",
        "                labels = batch.pop('labels')\n",
        "\n",
        "                logits, _ = model(**batch, return_loss=False)\n",
        "                loss = criterion(logits, labels)\n",
        "\n",
        "                total_val_loss += loss.item()\n",
        "                preds = torch.argmax(logits, dim=1)\n",
        "\n",
        "                all_preds.extend(preds.cpu().numpy())\n",
        "                all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "        avg_val_loss = total_val_loss / len(val_loader)\n",
        "        val_acc = accuracy_score(all_labels, all_preds)\n",
        "\n",
        "        training_stats['val_loss'].append(avg_val_loss)\n",
        "        training_stats['val_acc'].append(val_acc)\n",
        "\n",
        "        print(f\"Epoch {epoch+1} - Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
        "\n",
        "    return model, training_stats\n",
        "\n",
        "\n",
        "# ==============================\n",
        "# Execute Workflow\n",
        "# ==============================\n",
        "\n",
        "# Load and preprocess the dataset\n",
        "df = load_dataset(data_str)\n",
        "df = preprocess_dataset(df)\n",
        "df = download_images(df)\n",
        "\n",
        "# Split dataset\n",
        "train_df, val_df = train_test_split(df, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize the model and processor\n",
        "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "model = CLIPClassifier()\n",
        "\n",
        "# Create datasets and loaders\n",
        "train_dataset = FakeNewsDataset(train_df, processor)\n",
        "val_dataset = FakeNewsDataset(val_df, processor)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False)\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Train the model\n",
        "model, training_stats = train_vlm(model, train_loader, val_loader, device)\n"
      ],
      "metadata": {
        "id": "pJknavbMZoCq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "822db6fa-f332-4dc9-ade2-55dd2242f0ae"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading images: 100%|██████████| 3600/3600 [00:00<00:00, 23767.39it/s]\n",
            "Epoch 1/10 - Training: 100%|██████████| 630/630 [00:39<00:00, 16.13it/s]\n",
            "Epoch 1/10 - Validation: 100%|██████████| 270/270 [00:16<00:00, 16.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 - Train Loss: 0.6716, Val Loss: 0.6367, Val Acc: 0.7583\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/10 - Training: 100%|██████████| 630/630 [00:38<00:00, 16.47it/s]\n",
            "Epoch 2/10 - Validation: 100%|██████████| 270/270 [00:16<00:00, 16.71it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 - Train Loss: 0.5913, Val Loss: 0.5477, Val Acc: 0.7593\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/10 - Training: 100%|██████████| 630/630 [00:38<00:00, 16.43it/s]\n",
            "Epoch 3/10 - Validation: 100%|██████████| 270/270 [00:16<00:00, 16.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 - Train Loss: 0.5201, Val Loss: 0.4946, Val Acc: 0.7741\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/10 - Training: 100%|██████████| 630/630 [00:37<00:00, 16.63it/s]\n",
            "Epoch 4/10 - Validation: 100%|██████████| 270/270 [00:16<00:00, 16.60it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4 - Train Loss: 0.4844, Val Loss: 0.4715, Val Acc: 0.7750\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/10 - Training: 100%|██████████| 630/630 [00:37<00:00, 16.74it/s]\n",
            "Epoch 5/10 - Validation: 100%|██████████| 270/270 [00:15<00:00, 16.88it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5 - Train Loss: 0.4648, Val Loss: 0.4611, Val Acc: 0.7778\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6/10 - Training: 100%|██████████| 630/630 [00:37<00:00, 16.68it/s]\n",
            "Epoch 6/10 - Validation: 100%|██████████| 270/270 [00:15<00:00, 16.99it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6 - Train Loss: 0.4526, Val Loss: 0.4513, Val Acc: 0.7806\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7/10 - Training: 100%|██████████| 630/630 [00:37<00:00, 16.68it/s]\n",
            "Epoch 7/10 - Validation: 100%|██████████| 270/270 [00:16<00:00, 16.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7 - Train Loss: 0.4426, Val Loss: 0.4494, Val Acc: 0.7815\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8/10 - Training: 100%|██████████| 630/630 [00:37<00:00, 16.67it/s]\n",
            "Epoch 8/10 - Validation: 100%|██████████| 270/270 [00:16<00:00, 16.85it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8 - Train Loss: 0.4357, Val Loss: 0.4475, Val Acc: 0.7796\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9/10 - Training: 100%|██████████| 630/630 [00:37<00:00, 16.68it/s]\n",
            "Epoch 9/10 - Validation: 100%|██████████| 270/270 [00:16<00:00, 16.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9 - Train Loss: 0.4278, Val Loss: 0.4486, Val Acc: 0.7769\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10/10 - Training: 100%|██████████| 630/630 [00:37<00:00, 16.66it/s]\n",
            "Epoch 10/10 - Validation: 100%|██████████| 270/270 [00:16<00:00, 16.75it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10 - Train Loss: 0.4229, Val Loss: 0.4424, Val Acc: 0.7806\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def evaluate_model(model, processor, data_loader, device):\n",
        "    \"\"\"\n",
        "    Evaluate the model and print confusion matrix and metrics.\n",
        "\n",
        "    Args:\n",
        "    - model: Trained CLIP model\n",
        "    - processor: CLIPProcessor for preprocessing\n",
        "    - data_loader: DataLoader for validation/test dataset\n",
        "    - device: Device (CPU or GPU)\n",
        "\n",
        "    Returns:\n",
        "    - Confusion matrix and classification report\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    all_preds, all_labels = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in data_loader:\n",
        "            batch = {k: v.to(device) for k, v in batch.items()}\n",
        "\n",
        "            labels = batch.pop('labels')\n",
        "\n",
        "            # Perform inference\n",
        "            logits, _ = model(**batch, return_loss=False)\n",
        "\n",
        "            # Get predictions\n",
        "            preds = torch.argmax(logits, dim=1)\n",
        "\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    # Print accuracy and classification report\n",
        "    acc = accuracy_score(all_labels, all_preds)\n",
        "    print(f\"\\n✅ Accuracy: {acc:.4f}\\n\")\n",
        "\n",
        "    class_report = classification_report(all_labels, all_preds, target_names=[\"Fake\", \"Real\"])\n",
        "    print(\"\\n📊 Classification Report:\\n\")\n",
        "    print(class_report)\n",
        "\n",
        "    # Generate and display confusion matrix\n",
        "    conf_matrix = confusion_matrix(all_labels, all_preds)\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"Fake\", \"Real\"], yticklabels=[\"Fake\", \"Real\"])\n",
        "    plt.title(\"Confusion Matrix\")\n",
        "    plt.xlabel(\"Predicted Label\")\n",
        "    plt.ylabel(\"True Label\")\n",
        "    plt.show()\n",
        "\n",
        "    return conf_matrix, class_report\n",
        "\n",
        "  # Evaluate the model on the validation set\n",
        "conf_matrix, class_report = evaluate_model(model, processor, val_loader, device)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 836
        },
        "id": "_WH4Ktq-iH7b",
        "outputId": "ee9c2c2d-8ef3-4fe0-a362-4e34de363cc6"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✅ Accuracy: 0.7806\n",
            "\n",
            "\n",
            "📊 Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        Fake       0.76      0.79      0.77       513\n",
            "        Real       0.80      0.77      0.79       567\n",
            "\n",
            "    accuracy                           0.78      1080\n",
            "   macro avg       0.78      0.78      0.78      1080\n",
            "weighted avg       0.78      0.78      0.78      1080\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================\n",
        "# Save the Trained Model\n",
        "# ==============================\n",
        "model_save_path = \"./clip_fake_news_classifier1.pth\"\n",
        "\n",
        "# Save the model state_dict\n",
        "torch.save(model.state_dict(), model_save_path)\n",
        "print(f\"Model saved at {model_save_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AloQBt6bYDX4",
        "outputId": "1871258c-21a2-4b9f-b678-e820525a6a81"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved at ./clip_fake_news_classifier1.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================\n",
        "# Load the Saved Model\n",
        "# ==============================\n",
        "model_load_path = \"./clip_fake_news_classifier1.pth\"\n",
        "\n",
        "# Initialize the model\n",
        "model = CLIPClassifier()\n",
        "model.load_state_dict(torch.load(model_load_path))\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "print(f\"Model loaded from {model_load_path}\")\n"
      ],
      "metadata": {
        "id": "g2ig6YXS1_HN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9fa33a7a-e89c-437c-ebc4-e12aafd6d6d7"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded from ./clip_fake_news_classifier1.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(model, processor, text=None, image_path=None, device='cpu'):\n",
        "    \"\"\"\n",
        "    Perform inference on a single text, image, or both.\n",
        "\n",
        "    Args:\n",
        "    - model: Trained CLIP model\n",
        "    - processor: CLIPProcessor for preprocessing\n",
        "    - text: Claim text (str) [Optional]\n",
        "    - image_path: Path to the image file (str) [Optional]\n",
        "    - device: Device (CPU or GPU)\n",
        "\n",
        "    Returns:\n",
        "    - Prediction label (0 = Fake, 1 = Real) with confidence score\n",
        "    \"\"\"\n",
        "    # Ensure at least one input is provided\n",
        "    if not text and not image_path:\n",
        "        raise ValueError(\"Either text or image path must be provided.\")\n",
        "\n",
        "    # Prepare image or use placeholder\n",
        "    if image_path and os.path.exists(image_path) and image_path != 'N/A':\n",
        "        image = Image.open(image_path).convert(\"RGB\")\n",
        "    elif image_path:\n",
        "        print(f\"Image not found: {image_path}. Using placeholder.\")\n",
        "        image = Image.new('RGB', (224, 224), color='white')\n",
        "    else:\n",
        "        image = None  # No image provided\n",
        "\n",
        "    # --- Inference Logic ---\n",
        "    with torch.no_grad():\n",
        "        if text and image:\n",
        "            # Text + Image Inference\n",
        "            inputs = processor(\n",
        "                text=text,\n",
        "                images=image,\n",
        "                return_tensors=\"pt\",\n",
        "                padding=\"max_length\",\n",
        "                truncation=True,\n",
        "                max_length=77\n",
        "            ).to(device)\n",
        "\n",
        "            logits, _ = model(**inputs, return_loss=False)\n",
        "\n",
        "        elif text:\n",
        "            # Text-Only Inference\n",
        "            inputs = processor(\n",
        "                text=text,\n",
        "                return_tensors=\"pt\",\n",
        "                padding=\"max_length\",\n",
        "                truncation=True,\n",
        "                max_length=77\n",
        "            ).to(device)\n",
        "\n",
        "            text_features = model.clip.get_text_features(**inputs)\n",
        "            logits = model.classifier(text_features)\n",
        "\n",
        "        elif image:\n",
        "            # Image-Only Inference\n",
        "            inputs = processor(\n",
        "                images=image,\n",
        "                return_tensors=\"pt\"\n",
        "            ).to(device)\n",
        "\n",
        "            image_features = model.clip.get_image_features(**inputs)\n",
        "            logits = model.classifier(image_features)\n",
        "\n",
        "        else:\n",
        "            raise ValueError(\"Both text and image are missing!\")\n",
        "\n",
        "    # Get predictions and confidence scores\n",
        "    probs = torch.softmax(logits, dim=1)\n",
        "    pred_label = torch.argmax(probs, dim=1).item()\n",
        "    confidence = probs[0][pred_label].item()\n",
        "\n",
        "    label_str = \"Real\" if pred_label == 1 else \"Fake\"\n",
        "    return label_str, confidence\n"
      ],
      "metadata": {
        "id": "ZM_heRFta3BI"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"NASA confirmed water on Mars.\"\n",
        "image_path = \"./images/0.jpg\"\n",
        "\n",
        "label, confidence = predict(model, processor, text=text, image_path=image_path, device=device)\n",
        "print(f\"Prediction: {label}, Confidence: {confidence:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SIK38TeL-bnN",
        "outputId": "06ac9694-ffe4-438e-e438-39aef7ac06c6"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image not found: ./images/0.jpg. Using placeholder.\n",
            "Prediction: Fake, Confidence: 0.9285\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"The COVID-19 vaccine causes infertility.\"\n",
        "\n",
        "label, confidence = predict(model, processor, text=text, device=device)\n",
        "print(f\"Text-Only Prediction: {label}, Confidence: {confidence:.4f}\")\n"
      ],
      "metadata": {
        "id": "iXhja4il_K4v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4be5ae93-27da-46a1-ea0a-c61dcf640a7b"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text-Only Prediction: Fake, Confidence: 0.9999\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image_path = \"/content/images/1.jpg\"\n",
        "\n",
        "label, confidence = predict(model, processor, image_path=image_path, device=device)\n",
        "print(f\"Image-Only Prediction: {label}, Confidence: {confidence:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xoBz3IH7gxn5",
        "outputId": "db33fe8b-7359-4070-af0a-5e5024092742"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image-Only Prediction: Real, Confidence: 0.9718\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XcGdktILhIlW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}