{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ruiR9uU15c6g"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
        "from transformers import Trainer, TrainingArguments\n",
        "from torch.utils.data import Dataset\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Check if CUDA is available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3opuIBC35s8p",
        "outputId": "9f968f6c-3157-4c95-a7f6-38b1c9ba7242"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Define paths\n",
        "DATA_DIR = \"liar_dataset\"  # Change this to your LIAR dataset directory\n",
        "OUTPUT_DIR = \"distilbert_fake_news_model\"\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "# Define label mapping for LIAR dataset\n",
        "# LIAR has 6 labels: pants-fire, false, barely-true, half-true, mostly-true, true\n",
        "LABEL_MAP = {\n",
        "    'pants-fire': 0,\n",
        "    'false': 1,\n",
        "    'barely-true': 2,\n",
        "    'half-true': 3,\n",
        "    'mostly-true': 4,\n",
        "    'true': 5\n",
        "}\n"
      ],
      "metadata": {
        "id": "626sHQ8L65HD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Dataset class for LIAR\n",
        "class LiarDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_length=128):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        encoding = self.tokenizer(\n",
        "            text,\n",
        "            max_length=self.max_length,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        # Convert batch dimension tensor to regular tensor\n",
        "        encoding = {k: v.squeeze(0) for k, v in encoding.items()}\n",
        "        encoding['labels'] = torch.tensor(label)\n",
        "\n",
        "        return encoding\n"
      ],
      "metadata": {
        "id": "8Adgj9pP7Bgm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def load_liar_data(data_dir):\n",
        "    # LIAR dataset has these columns:\n",
        "    # id, label, statement, subject, speaker, job_title, state_info, party_affiliation,\n",
        "    # barely_true_counts, false_counts, half_true_counts, mostly_true_counts, pants_on_fire_counts, context\n",
        "\n",
        "    # Loading training data\n",
        "    train_path = os.path.join(data_dir, \"/content/train.tsv\")\n",
        "    train_df = pd.read_csv(train_path, sep='\\t', header=None)\n",
        "\n",
        "    # Loading validation data\n",
        "    val_path = os.path.join(data_dir, \"/content/valid.tsv\")\n",
        "    val_df = pd.read_csv(val_path, sep='\\t', header=None)\n",
        "\n",
        "    # Loading test data\n",
        "    test_path = os.path.join(data_dir, \"/content/test.tsv\")\n",
        "    test_df = pd.read_csv(test_path, sep='\\t', header=None)\n",
        "\n",
        "    # Extract relevant columns (label and statement)\n",
        "    train_texts = train_df[2].tolist()  # statement is 3rd column (index 2)\n",
        "    train_labels = train_df[1].map(LABEL_MAP).tolist()  # label is 2nd column (index 1)\n",
        "\n",
        "    val_texts = val_df[2].tolist()\n",
        "    val_labels = val_df[1].map(LABEL_MAP).tolist()\n",
        "\n",
        "    test_texts = test_df[2].tolist()\n",
        "    test_labels = test_df[1].map(LABEL_MAP).tolist()\n",
        "\n",
        "    print(f\"Train examples: {len(train_texts)}\")\n",
        "    print(f\"Validation examples: {len(val_texts)}\")\n",
        "    print(f\"Test examples: {len(test_texts)}\")\n",
        "\n",
        "    return train_texts, train_labels, val_texts, val_labels, test_texts, test_labels\n"
      ],
      "metadata": {
        "id": "uQ2Ymyg07Jxf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def main():\n",
        "    # Load tokenizer and model\n",
        "    model_name = \"distilbert-base-uncased\"\n",
        "    tokenizer = DistilBertTokenizer.from_pretrained(model_name)\n",
        "\n",
        "    # Load LIAR dataset\n",
        "    train_texts, train_labels, val_texts, val_labels, test_texts, test_labels = load_liar_data(DATA_DIR)\n",
        "\n",
        "    # Create datasets\n",
        "    train_dataset = LiarDataset(train_texts, train_labels, tokenizer)\n",
        "    val_dataset = LiarDataset(val_texts, val_labels, tokenizer)\n",
        "    test_dataset = LiarDataset(test_texts, test_labels, tokenizer)\n",
        "\n",
        "    # Load pre-trained model with classification head\n",
        "    num_labels = len(LABEL_MAP)\n",
        "    model = DistilBertForSequenceClassification.from_pretrained(\n",
        "        model_name,\n",
        "        num_labels=num_labels\n",
        "    )\n",
        "\n",
        "    # Move model to device\n",
        "    model = model.to(device)\n",
        "\n",
        "    # Define training arguments\n",
        "    training_args = TrainingArguments(\n",
        "      output_dir=OUTPUT_DIR,\n",
        "      num_train_epochs=2,  # Reduce epochs (faster but may affect performance)\n",
        "      per_device_train_batch_size=32,  # Increase batch size (if GPU memory allows)\n",
        "      per_device_eval_batch_size=128,  # Increase for faster evaluation\n",
        "      gradient_accumulation_steps=2,  # Accumulate gradients to simulate larger batch sizes\n",
        "      warmup_steps=0,  # Remove warmup unless necessary\n",
        "      weight_decay=0.01,\n",
        "      logging_dir=os.path.join(OUTPUT_DIR, \"logs\"),\n",
        "      logging_steps=500,  # Reduce logging frequency for faster training\n",
        "      eval_steps=1000,  # Evaluate less frequently\n",
        "      save_steps=1000,  # Save checkpoints less often\n",
        "      evaluation_strategy=\"epoch\",  # Evaluate only at the end of each epoch\n",
        "      save_strategy=\"epoch\",  # Save only at epoch level\n",
        "      load_best_model_at_end=True,\n",
        "      metric_for_best_model=\"accuracy\",\n",
        "      fp16=True,  # Enable mixed precision training (faster on GPUs)\n",
        "      dataloader_num_workers=4,  # Use multiple workers for data loading\n",
        "\n",
        "    )\n",
        "\n",
        "    # Define compute_metrics function\n",
        "    def compute_metrics(eval_pred):\n",
        "        predictions, labels = eval_pred\n",
        "        predictions = np.argmax(predictions, axis=1)\n",
        "        accuracy = accuracy_score(labels, predictions)\n",
        "        return {\"accuracy\": accuracy}\n",
        "\n",
        "    # Initialize Trainer\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=val_dataset,\n",
        "        compute_metrics=compute_metrics,\n",
        "    )\n",
        "\n",
        "    # Train the model\n",
        "    print(\"Starting training...\")\n",
        "    trainer.train()\n",
        "\n",
        "    # Save the model\n",
        "    model.save_pretrained(os.path.join(OUTPUT_DIR, \"final_model\"))\n",
        "    tokenizer.save_pretrained(os.path.join(OUTPUT_DIR, \"tokenizer\"))\n",
        "\n",
        "    # Evaluate on test set\n",
        "    print(\"Evaluating on test set...\")\n",
        "    trainer.eval_dataset = test_dataset\n",
        "    test_results = trainer.evaluate()\n",
        "    print(f\"Test results: {test_results}\")\n",
        "\n",
        "    # Generate predictions for a more detailed evaluation\n",
        "    test_predictions = trainer.predict(test_dataset)\n",
        "    preds = np.argmax(test_predictions.predictions, axis=1)\n",
        "\n",
        "    # Reverse label mapping for readable report\n",
        "    reverse_label_map = {v: k for k, v in LABEL_MAP.items()}\n",
        "    label_names = [reverse_label_map[i] for i in range(len(LABEL_MAP))]\n",
        "\n",
        "    # Print classification report\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(classification_report(test_labels, preds, target_names=label_names))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 663
        },
        "id": "5gAP6Qif7PcY",
        "outputId": "1157a65a-e78c-4f11-bdaf-a86f692d613e"
      },
      "execution_count": 18,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train examples: 10240\n",
            "Validation examples: 1284\n",
            "Test examples: 1267\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting training...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='301' max='320' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [301/320 3:09:12 < 12:01, 0.03 it/s, Epoch 1.88/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.692194</td>\n",
              "      <td>0.262461</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='320' max='320' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [320/320 3:26:13, Epoch 2/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.692194</td>\n",
              "      <td>0.262461</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>No log</td>\n",
              "      <td>1.685267</td>\n",
              "      <td>0.257009</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating on test set...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test results: {'eval_loss': 1.677376389503479, 'eval_accuracy': 0.26203630623520124, 'eval_runtime': 244.2125, 'eval_samples_per_second': 5.188, 'eval_steps_per_second': 0.041, 'epoch': 2.0}\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "  pants-fire       0.00      0.00      0.00        92\n",
            "       false       0.27      0.38      0.31       249\n",
            " barely-true       0.28      0.08      0.12       212\n",
            "   half-true       0.25      0.32      0.28       265\n",
            " mostly-true       0.26      0.53      0.35       241\n",
            "        true       0.31      0.04      0.08       208\n",
            "\n",
            "    accuracy                           0.26      1267\n",
            "   macro avg       0.23      0.22      0.19      1267\n",
            "weighted avg       0.25      0.26      0.22      1267\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
        "\n",
        "# Define paths to model and tokenizer\n",
        "model_path = \"/content/distilbert_fake_news_model/final_model\"  # Adjust if different\n",
        "tokenizer_path = \"/content/distilbert_fake_news_model/tokenizer\"\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = DistilBertTokenizer.from_pretrained(tokenizer_path)\n",
        "\n",
        "# Load trained model\n",
        "model = DistilBertForSequenceClassification.from_pretrained(model_path)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "# Define label mapping\n",
        "LABEL_MAP = {\n",
        "    0: 'pants-fire',\n",
        "    1: 'false',\n",
        "    2: 'barely-true',\n",
        "    3: 'half-true',\n",
        "    4: 'mostly-true',\n",
        "    5: 'true'\n",
        "}\n",
        "\n",
        "def predict(text):\n",
        "    \"\"\"Function to predict the class of a given text.\"\"\"\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
        "    inputs = {key: value.to(device) for key, value in inputs.items()}  # Move to device\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.logits\n",
        "        predicted_label = torch.argmax(logits, dim=1).item()\n",
        "\n",
        "    return LABEL_MAP[predicted_label]\n",
        "\n",
        "# Example Usage\n",
        "text = \"NASA confirms that the moon is not made of cheese.\"\n",
        "prediction = predict(text)\n",
        "print(f\"Prediction: {prediction}\")\n"
      ],
      "metadata": {
        "id": "Wwtb__PlF8_K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "959c413c-1e36-47d7-f2b7-2239742da347"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction: false\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pSbClm9V7yi5"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "g-oMLd3x0zO7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
